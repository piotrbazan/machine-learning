{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(size, item):\n",
    "    o = np.zeros((size, 1))\n",
    "    o[item] = 1\n",
    "    return o\n",
    "\n",
    "def softmax(y):\n",
    "    return np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "def zeros_like(*args):\n",
    "    return (np.zeros_like(a) for a in args)\n",
    "\n",
    "def clip(*args):\n",
    "    for a in args:\n",
    "        np.clip(a, -5, 5, out=a) # clip to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wx = np.random.randn(hidden_size, input_size) * .01\n",
    "        self.wh = np.random.randn(hidden_size, hidden_size)* .01\n",
    "        self.wy = np.random.randn(output_size, hidden_size)* .01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        self.h = np.zeros((hidden_size, 1))\n",
    "        # adagrad params\n",
    "        self.mbh, self.mby, self.mwx, self.mwh, self.mwy = zeros_like(self.bh, self.by, self.wx, self.wh, self.wy)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x_, y_, h_, logit_, p_, loss = {}, {}, {}, {}, {}, 0\n",
    "        h_[-1] = np.copy(self.h)\n",
    "        for i in range(len(x)):\n",
    "            x_[i] = oneHot(self.input_size, x[i])\n",
    "            y_[i] = oneHot(self.output_size, y[i])\n",
    "            h_[i] = np.tanh(np.dot(self.wx, x_[i]) + np.dot(self.wh, h_[i - 1]) + self.bh)\n",
    "            logit_[i] = np.dot(self.wy, h_[i]) + self.by\n",
    "            p_[i] = softmax(logit_[i])\n",
    "            loss += np.sum(-1 * y_[i] * np.log(p_[i]) - (1 - y_[i]) * np.log(1 - p_[i]))\n",
    "            #loss += np.sum(-1 * y_[i] * np.log(p_[i]))\n",
    "        self.forward_stash = x_, y_, h_, logit_, p_, loss\n",
    "        \n",
    "    def backward(self):\n",
    "        x_, y_, h_, logit_, p_, loss = self.forward_stash\n",
    "        dbh, dby, dwx, dwh, dwy, dhnext = zeros_like(self.bh, self.by, self.wx, self.wh, self.wy, self.h)\n",
    "        for i in reversed(range(len(x_))):\n",
    "            dy = p_[i] - y_[i]\n",
    "            dby += dy            \n",
    "            dwy += np.dot(dy, h_[i].T)\n",
    "            dh = np.dot(self.wy.T, dy) + dhnext\n",
    "            dtanh = (1 - h_[i]**2) * dh\n",
    "            dbh += dtanh\n",
    "            dwx += np.dot(dtanh, x_[i].T)\n",
    "            dwh += np.dot(dtanh, h_[i-1].T)\n",
    "            dhnext = np.dot(self.wh.T, dtanh)\n",
    "        clip(dbh, dby, dwx, dwh, dwy)    \n",
    "        self.backward_stash = dwx, dwh, dwy, dbh, dby\n",
    "        self.h = h_[len(x_) - 1]\n",
    "        return loss\n",
    "    \n",
    "    def grad_update(self, learning_rate):        \n",
    "        dwx, dwh, dwy, dbh, dby = self.backward_stash\n",
    "        for param, dparam, mem in zip([self.wx, self.wh, self.wy, self.bh, self.by], \n",
    "                                      [dwx, dwh, dwy, dbh, dby], \n",
    "                                      [self.mwx, self.mwh, self.mwy, self.mbh, self.mby]):\n",
    "            mem += dparam ** 2\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    def sample(self, seed, N):\n",
    "        x = oneHot(self.input_size, seed)\n",
    "        res, h = [], np.copy(self.h)\n",
    "        for i in range(N):\n",
    "            h = np.tanh(np.dot(self.wx, x) + np.dot(self.wh, h) + self.bh)\n",
    "            y = np.dot(self.wy, h) + self.by\n",
    "            logit = softmax(y)\n",
    "            pick = np.random.choice(range(self.input_size), p=logit.ravel())\n",
    "            x = oneHot(self.input_size, pick)\n",
    "            res.append(pick)\n",
    "        return res  \n",
    "    \n",
    "    def reset_memory(self):\n",
    "        self.h = np.zeros((self.hidden_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(iterations = 2000, filename = 'input.txt', seq_length = 25, hidden_size = 100, learning_rate = 1e-1):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    chars = list(set(data))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "    print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    rnn = RNN(vocab_size, hidden_size, vocab_size)\n",
    "\n",
    "    n, p = 0, 0\n",
    "    smooth_loss = -np.log(1./ vocab_size) * seq_length # loss at iteration 0\n",
    "    for n in range(iterations):\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if  p + seq_length + 1 >= len(data) or n == 0: \n",
    "            rnn.reset_memory()\n",
    "            p = 0 # go from start of data\n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "        if n % 1000 == 0:\n",
    "            sample_ix = rnn.sample(inputs[0], 200)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "        rnn.forward(inputs, targets)\n",
    "        loss = rnn.backward()\n",
    "        rnn.grad_update(learning_rate)\n",
    "        \n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        if n % 1000 == 0: \n",
    "            print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "            \n",
    "        p += seq_length # move data pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 50 characters, 21 unique.\n",
      "----\n",
      " odoejTjeueuljwr!ih!ousuurwskjuhfH.nestwudhohi jfutrhjTwrlsrfT.jrusohj wrw\n",
      "!.ewwr\n",
      "rhkkj hhtnudrshw.hh wnH rijr.wjkfH\n",
      "tn\n",
      "Hoo!.f! n\n",
      "!Hs\n",
      "ikel !lf\n",
      "k.uwhtTnHertjuljklHuhhdhHiui Huissuhjeijr!iTruwTrjtodir.e  \n",
      "----\n",
      "iter 0, loss: 76.137460\n",
      "----\n",
      " Hello world for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thoslo world for rnn! Thisoworld for rnn! Thisow rld for rn \n",
      "----\n",
      "iter 1000, loss: 30.071132\n",
      "----\n",
      " Hello world for rnn! Thisiwowld for rnn! Thisow rld for rnn! Thisowlwld for rnn! Thisor rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisowornd fThisld world fo \n",
      "----\n",
      "iter 2000, loss: 11.105116\n"
     ]
    }
   ],
   "source": [
    "run('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 50 characters, 21 unique.\n",
      "----\n",
      " odoejTjeueuljwr!ih!ousuurwskjuhfH.nestwudhohi jfutrhjTwrlsrfT.jrusohj wrw\n",
      "!.ewwr\n",
      "rhkkj hhtnudrshw.hh wnH rijr.wjkfH\n",
      "tn\n",
      "Hoo!.f! n\n",
      "!Hs\n",
      "ikel !lf\n",
      "k.uwhtTnHertjuljklHuhhdhHiui Huissuhjeijr!iTruwTrjtodir.e  \n",
      "----\n",
      "iter 0, loss: 76.137460\n",
      "----\n",
      " Hello world for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thisoworld for rnn! Thoslo world for rnn! Thisoworld for rnn! Thisow rld for rn \n",
      "----\n",
      "iter 1000, loss: 30.071132\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8280576821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 1, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(5, 10, 5)\n",
    "x = np.array([1,1,1])\n",
    "y = np.array([1,1,1])\n",
    "rnn.reset_memory()\n",
    "rnn.forward(x, y)\n",
    "print rnn.backward()\n",
    "rnn.grad_update(1e-1)\n",
    "rnn.sample(1, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
